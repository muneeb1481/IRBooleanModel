{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a717692a-bc9a-4704-8b41-b4b29473c2f1",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca29c597-564c-48e6-9371-d75604e4e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f1b7ff0a-97c7-4448-8bd8-d815d96e53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert text to lowercase\n",
    "def convert_to_lowercase(directory):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        content = content.lower()\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"All documents have been converted to lowercase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c8c6a7cc-2448-442d-a544-e008a0bd95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load stopwords from a file\n",
    "def load_stopwords(stopword_file):\n",
    "    stopwords = []\n",
    "    with open(stopword_file, 'r') as file:\n",
    "        for line in file:\n",
    "            cleaned_word = line.strip()\n",
    "            if cleaned_word:\n",
    "                stopwords.append(cleaned_word)\n",
    "    print(\"Stopwords array created!\")\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4fa19c7-1ec2-453f-b662-a12630524bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords from documents\n",
    "def remove_stopwords(directory, stopwords):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        for word in stopwords:\n",
    "            content = content.replace(f' {word} ', ' ')\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"Stopwords removed from all documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b7139df-d145-4420-855a-258872c52031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuations from documents\n",
    "def remove_punctuations(directory):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        content = re.sub(r'[^A-Za-z]+', ' ', content)\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"Punctuations removed from all documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6164553e-b891-45b8-9e18-3da81c04ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Porter Stemmer to documents\n",
    "def apply_stemming(directory):\n",
    "    stemmer = PorterStemmer()\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        tokens = word_tokenize(content)\n",
    "        stemmed_tokens = [stemmer.stem(word) for word in tokens if len(word) > 1]\n",
    "        stemmed_content = ' '.join(stemmed_tokens)\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(stemmed_content)\n",
    "    print(\"Porter Stemmer applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c5afc40-8a44-43a0-94df-a0c7aa595039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to execute all preprocessing steps\n",
    "def preprocess_documents(directory, stopword_file):\n",
    "    convert_to_lowercase(directory)\n",
    "    stopwords = load_stopwords(stopword_file)\n",
    "    remove_stopwords(directory, stopwords)\n",
    "    remove_punctuations(directory)\n",
    "    apply_stemming(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "25facef0-42c8-4a32-a299-abf1ebbffcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been converted to lowercase.\n",
      "Stopwords array created!\n",
      "Stopwords removed from all documents!\n",
      "Punctuations removed from all documents!\n",
      "Porter Stemmer applied successfully!\n"
     ]
    }
   ],
   "source": [
    "abstract='Abstracts'\n",
    "stopword='Stopword-List.txt'\n",
    "preprocess_documents(abstract, stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3d0b7-3b93-4c50-8696-3beb3266130f",
   "metadata": {},
   "source": [
    "# Creating Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06d78314-fe2b-465b-97c2-30c5955e38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def get_files(dir):\n",
    "    files = os.listdir(dir)\n",
    "    pairs = [(int(f.replace('.txt', '')), f) for f in files]\n",
    "    pairs.sort()\n",
    "    return [p[1] for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cea5e228-e7f6-4e04-9c18-a03bb65999f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index(index, file):\n",
    "    with open(file, 'w') as f:\n",
    "        for term, docs in index.items():\n",
    "            f.write(f\"{term} : {docs}\\n\")\n",
    "    print(f\"Index saved to {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2bdf9165-1805-4c94-a7e9-c6d2a97315a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa5e16-e135-4c45-9abc-81bb6da8408d",
   "metadata": {},
   "source": [
    "# Creating Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b730a76f-0963-46c1-82b7-c79fbc326dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inv_index(dir, files):\n",
    "    inv_index = {}\n",
    "    for f in files:\n",
    "        with open(os.path.join(dir, f), 'r') as file:\n",
    "            text = file.read()\n",
    "        words = word_tokenize(text)\n",
    "        doc_id = int(f.replace('.txt', ''))\n",
    "        for word in words:\n",
    "            if word not in inv_index:\n",
    "                inv_index[word] = [doc_id]\n",
    "            elif doc_id not in inv_index[word]:\n",
    "                inv_index[word].append(doc_id)\n",
    "    return dict(sorted(inv_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b4999bf6-6332-46f0-b5e8-eef9b6891ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms in inverted index: 4227\n",
      "Index saved to inverted_index.txt\n"
     ]
    }
   ],
   "source": [
    "inv_index = build_inv_index(abstract, files)\n",
    "print(\"Terms in inverted index:\", len(inv_index))\n",
    "save_index(inv_index, \"inverted_index.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "91c7aa22-528e-47f8-b35a-6d3e8c9b69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a term in the inverted index.\n",
    "def search(index, term):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = stemmer.stem(term)\n",
    "    if stemmed in index:\n",
    "        print(f\"Results for '{term}': {index[stemmed]}\")\n",
    "    else:\n",
    "        print(f\"No results for '{term}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "43626a77-d02f-4144-9076-9f370a516ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 'autoencoders': [187, 273, 279, 325, 333, 405]\n"
     ]
    }
   ],
   "source": [
    "search(inv_index, \"autoencoders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb9cce-be03-4236-b8b0-7f7429566798",
   "metadata": {},
   "source": [
    "# Creating Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "57a1e039-0292-4c13-98ce-17c2411ef373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pos_index(dir, files):\n",
    "    pos_index = {}\n",
    "    for f in files:\n",
    "        doc_id = int(f.replace('.txt', ''))\n",
    "        with open(os.path.join(dir, f), 'r') as file:\n",
    "            text = file.read()\n",
    "        terms = word_tokenize(text)\n",
    "        pos = 0\n",
    "        for term in terms:\n",
    "            if term not in pos_index:\n",
    "                pos_index[term] = {}\n",
    "            if doc_id not in pos_index[term]:\n",
    "                pos_index[term][doc_id] = []\n",
    "            pos_index[term][doc_id].append(pos)\n",
    "            pos += 1\n",
    "    return dict(sorted(pos_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "38dd7805-719b-45d6-acae-1c80b42a2b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms in positional index: 4227\n",
      "Index saved to positional_index.txt\n"
     ]
    }
   ],
   "source": [
    "pos_index = build_pos_index(abstract, files)\n",
    "print(\"Terms in positional index:\", len(pos_index))\n",
    "save_index(pos_index, \"positional_index.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
